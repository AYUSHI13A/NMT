{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "78be6e5f",
   "metadata": {},
   "source": [
    "# Encoder and Decoder Model\n",
    "\n",
    "- In this file , I am going to train and save the trained Encoder and Decoder model .\n",
    "- As well as , I am going to convert sentence of Text to Numerical sequence and I am going to create function for this conversion. Save the function Convert_data into .py format.\n",
    "- I am going to use this NMT_convert_data that convert data into numerical sequence in next file . Which will be based on Prediction."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54d0982a",
   "metadata": {},
   "source": [
    "### Importing Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "03b5e235",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import os\n",
    "import pandas as pd\n",
    "import csv\n",
    "import tensorflow.python as tf\n",
    "import keras\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "import json\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Embedding , Bidirectional , LSTM , Dense\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "from tensorflow.keras.layers import Dropout\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tensorflow.keras.utils import plot_model\n",
    "from keras.models import Model\n",
    "from keras.layers import Input, Embedding, LSTM, Bidirectional, RepeatVector, TimeDistributed, Dense , Concatenate\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dab2eae6",
   "metadata": {},
   "source": [
    "### Define path for both training data and testing data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "9a7db2b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data_path = r'C:\\Users\\ayush\\jupyter Notebook\\NMT\\data\\Training_data.csv'\n",
    "test_data_path = r'C:\\Users\\ayush\\jupyter Notebook\\NMT\\data\\Testing_data.csv'\n",
    "\n",
    "train_data_path = os.path.join(train_data_path)\n",
    "test_data_path = os.path.join(test_data_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7efc9f27",
   "metadata": {},
   "source": [
    "## Loading the train data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "eb05346f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Source</th>\n",
       "      <th>Target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>145803</th>\n",
       "      <td>qui va là</td>\n",
       "      <td>&lt;start&gt;who goes there&lt;end&gt;</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>54895</th>\n",
       "      <td>me connaissezvous le moins du monde</td>\n",
       "      <td>&lt;start&gt;do not you know me at all&lt;end&gt;</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31284</th>\n",
       "      <td>je reviens bientôt</td>\n",
       "      <td>&lt;start&gt;i will be back really soon&lt;end&gt;</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>153756</th>\n",
       "      <td>jadore jouer du chopin</td>\n",
       "      <td>&lt;start&gt;i love playing chopin&lt;end&gt;</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>134437</th>\n",
       "      <td>tu ne te rappelles pas  je ne loublierai jamais</td>\n",
       "      <td>&lt;start&gt;you cannot remember it and i will never...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                 Source  \\\n",
       "145803                                       qui va là    \n",
       "54895              me connaissezvous le moins du monde    \n",
       "31284                                je reviens bientôt   \n",
       "153756                           jadore jouer du chopin   \n",
       "134437  tu ne te rappelles pas  je ne loublierai jamais   \n",
       "\n",
       "                                                   Target  \n",
       "145803                         <start>who goes there<end>  \n",
       "54895               <start>do not you know me at all<end>  \n",
       "31284              <start>i will be back really soon<end>  \n",
       "153756                  <start>i love playing chopin<end>  \n",
       "134437  <start>you cannot remember it and i will never...  "
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## reading csv file\n",
    "training_data = pd.read_csv(train_data_path , encoding = 'UTF-8')\n",
    "training_data.sample(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1c39de8",
   "metadata": {},
   "source": [
    "### Preprocessing the data for the encoder - decoder model\n",
    "- The model requires the data in the form of numerical sequences. Since the model requires numerical data.\n",
    "- Although we have text data, but we need to convert the SEQUENCE OF TEXT to NUMERICAL SEQUENCES.\n",
    "- In this project we will do the conversion word wise."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f9c7974",
   "metadata": {},
   "source": [
    "#### Defining a function, which will convert text sequences to numerical, sequences."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "e0e4bf88",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_numeric_sequences(text_sequences) :\n",
    "    ## create a tokenizer -> this will map each word to a number.\n",
    "    ## defining the num_words -> this indicates that we need maximum of these words only to process.\n",
    "    num_words = 1500\n",
    "    ## initializing the tokenizer\n",
    "    ## defining OOV -> this handles a word if it is out of vocabulary\n",
    "    token = Tokenizer(num_words=num_words, oov_token=\"<UKN>\")\n",
    "    ## create the word_index -> word_index is the dictionary which maps the words to a numeric value.\n",
    "    token.fit_on_texts(text_sequences)\n",
    "    ## saving word_index.\n",
    "    word_index = token.word_index\n",
    "    ## limiting the words to num_words in the dictionary.\n",
    "    word_indices = {word: index for word, index in token.word_index.items() if index <= num_words}\n",
    "    ## converting sequences.\n",
    "    num_sequences = token.texts_to_sequences(text_sequences)\n",
    "    ## define vocabulary size -> this is the size of the word_index.\n",
    "    ## we are incrementing it by 1 because the indexing starts with 1.\n",
    "    vocab_size = len(word_indices) + 1\n",
    "    \n",
    "    return num_sequences, token, vocab_size , word_indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "92e94e73",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_pad_sequeces(source_num_sequenecs, target_num_sequences, max_common_length = None) :\n",
    "    \n",
    "    '''This function returns padded sequences and maximun common length.'''\n",
    "    \n",
    "    ## finding out the maximum lenght of source sequences.\n",
    "    source_max_len = max([len(seq) for seq in source_num_sequenecs])\n",
    "    ## finding out the maximum lenght of target sequences.\n",
    "    target_max_len = max([len(seq) for seq in target_num_sequences])    \n",
    "    ## finding the common maximum length.\n",
    "    COMMON_MAX_LENGTH = max(source_max_len, target_max_len)\n",
    "    ## checking if the argument already has a max_common_length.\n",
    "    if max_common_length != None :\n",
    "        COMMON_MAX_LENGTH = max_common_length\n",
    "    ## pad the sequences.\n",
    "    source_padded_sequences = pad_sequences(source_num_sequenecs, maxlen = COMMON_MAX_LENGTH, padding='post')\n",
    "    target_padded_sequences = pad_sequences(target_num_sequences, maxlen = COMMON_MAX_LENGTH, padding='post')\n",
    "    \n",
    "    return source_padded_sequences, target_padded_sequences, COMMON_MAX_LENGTH"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "c714f157",
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_data(source, target, max_common_length = None) :\n",
    "    \n",
    "    '''This function returns the complete converted set.'''\n",
    "    \n",
    "    ## get numerical sequences.\n",
    "    source_num_sequences, source_token, source_vocab_size , word_indices_source = get_numeric_sequences(source)\n",
    "    target_num_sequences, target_token, target_vocab_size , word_indices_target = get_numeric_sequences(target)\n",
    "    ## get padded sequences.\n",
    "    source_padded_sequences, target_padded_sequences, COMMON_MAX_LENGTH = get_pad_sequeces(source_num_sequences, target_num_sequences, max_common_length = max_common_length)\n",
    "    \n",
    "    return source_padded_sequences, target_padded_sequences, COMMON_MAX_LENGTH, source_vocab_size, target_vocab_size , word_indices_source , word_indices_target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "0828e9da",
   "metadata": {},
   "outputs": [],
   "source": [
    "source = training_data.Source\n",
    "target = training_data.Target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "2ac725f8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((172352,), (172352,))"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "source.shape , target.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "80c4d99a",
   "metadata": {},
   "outputs": [],
   "source": [
    "source_sequences, target_sequences, COMMON_MAX_LENGTH, source_vocab_size, target_vocab_size , word_indices_source , word_indices_target = convert_data(source, target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "b1acb1ee",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[  11,   17,    1, ...,    0,    0,    0],\n",
       "       [  44,    2,    1, ...,    0,    0,    0],\n",
       "       [ 900,    3,   27, ...,    0,    0,    0],\n",
       "       ...,\n",
       "       [   1,    7, 1490, ...,    0,    0,    0],\n",
       "       [  19,    1,    1, ...,    0,    0,    0],\n",
       "       [  47,  700,    1, ...,    0,    0,    0]])"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "source_sequences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "8492a14f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[   2,   13,  143, ...,    0,    0,    0],\n",
       "       [   2,   69,    4, ...,    0,    0,    0],\n",
       "       [   2,  190,    6, ...,    0,    0,    0],\n",
       "       ...,\n",
       "       [   2,  286,  103, ...,    0,    0,    0],\n",
       "       [   2,    7, 1395, ...,    0,    0,    0],\n",
       "       [   2,   12,  530, ...,    0,    0,    0]])"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "target_sequences"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "819dd929",
   "metadata": {},
   "source": [
    "## Building Encoder - Decoder Model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c6abc00",
   "metadata": {},
   "source": [
    "### Defining Encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "05b473a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "## 1st layer is the input layer.\n",
    "encoder_input = Input(shape=(None,))\n",
    "## 2nd layer is the embedding layer\n",
    "encoder_embd = Embedding(source_vocab_size,100, mask_zero=True)(encoder_input)\n",
    "## 3rd layer is the LSTM Bideirectional layer.\n",
    "## The biderectional is being added because it will capture sequence information from both past and future.\n",
    "encoder_lstm = Bidirectional(LSTM(32, return_state=True))\n",
    "## getting output from encoder.\n",
    "encoder_output, forw_state_h, forw_state_c, back_state_h, back_state_c = encoder_lstm(encoder_embd)\n",
    "state_h_final = Concatenate()([forw_state_h, back_state_h])\n",
    "state_c_final = Concatenate()([forw_state_c, back_state_c])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "bc2207d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Now take only states and create context vector\n",
    "encoder_states= [state_h_final, state_c_final]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6de6ca13",
   "metadata": {},
   "source": [
    "### Defining Decoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "918a9b3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "decoder_input = Input(shape=(None,))\n",
    "# For zero padding we have added +1 in marathi vocab size\n",
    "decoder_embd = Embedding(target_vocab_size, 100, mask_zero=True)\n",
    "decoder_embedding= decoder_embd(decoder_input)\n",
    "# We used bidirectional layer above so we have to double units of this lstm\n",
    "decoder_lstm = LSTM(64, return_state=True,return_sequences=True )\n",
    "# just take output of this decoder dont need self states\n",
    "decoder_outputs, _, _= decoder_lstm(decoder_embedding, initial_state=encoder_states)\n",
    "# here this is going to predicct so we can add dense layer here\n",
    "# here we want to convert predicted numbers into probability so use softmax\n",
    "decoder_dense= Dense(target_vocab_size, activation='softmax')\n",
    "# We will again feed predicted output into decoder to predict its next word\n",
    "decoder_outputs = decoder_dense(decoder_outputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "ad491d99",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Model([encoder_input, decoder_input], decoder_outputs)\n",
    "model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "75bf7c00",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_2\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                Output Shape                 Param #   Connected to                  \n",
      "==================================================================================================\n",
      " input_5 (InputLayer)        [(None, None)]               0         []                            \n",
      "                                                                                                  \n",
      " embedding_4 (Embedding)     (None, None, 100)            150100    ['input_5[0][0]']             \n",
      "                                                                                                  \n",
      " input_6 (InputLayer)        [(None, None)]               0         []                            \n",
      "                                                                                                  \n",
      " bidirectional_2 (Bidirecti  [(None, 64),                 34048     ['embedding_4[0][0]']         \n",
      " onal)                        (None, 32),                                                         \n",
      "                              (None, 32),                                                         \n",
      "                              (None, 32),                                                         \n",
      "                              (None, 32)]                                                         \n",
      "                                                                                                  \n",
      " embedding_5 (Embedding)     (None, None, 100)            150100    ['input_6[0][0]']             \n",
      "                                                                                                  \n",
      " concatenate_4 (Concatenate  (None, 64)                   0         ['bidirectional_2[0][1]',     \n",
      " )                                                                   'bidirectional_2[0][3]']     \n",
      "                                                                                                  \n",
      " concatenate_5 (Concatenate  (None, 64)                   0         ['bidirectional_2[0][2]',     \n",
      " )                                                                   'bidirectional_2[0][4]']     \n",
      "                                                                                                  \n",
      " lstm_5 (LSTM)               [(None, None, 64),           42240     ['embedding_5[0][0]',         \n",
      "                              (None, 64),                            'concatenate_4[0][0]',       \n",
      "                              (None, 64)]                            'concatenate_5[0][0]']       \n",
      "                                                                                                  \n",
      " dense_2 (Dense)             (None, None, 1501)           97565     ['lstm_5[0][0]']              \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 474053 (1.81 MB)\n",
      "Trainable params: 474053 (1.81 MB)\n",
      "Non-trainable params: 0 (0.00 Byte)\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "aeb89aaf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "You must install pydot (`pip install pydot`) and install graphviz (see instructions at https://graphviz.gitlab.io/download/) for plot_model to work.\n"
     ]
    }
   ],
   "source": [
    "plot_model(model , show_shapes = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "32d257e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder_input_data = source_sequences\n",
    "decoder_input_data = target_sequences[:,:-1]\n",
    "decoder_target_data = target_sequences[:,1:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53aa1f05",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/15\n",
      "WARNING:tensorflow:From C:\\Users\\ayush\\anaconda3\\Lib\\site-packages\\keras\\src\\utils\\tf_utils.py:492: The name tf.ragged.RaggedTensorValue is deprecated. Please use tf.compat.v1.ragged.RaggedTensorValue instead.\n",
      "\n",
      "WARNING:tensorflow:From C:\\Users\\ayush\\anaconda3\\Lib\\site-packages\\keras\\src\\engine\\base_layer_utils.py:384: The name tf.executing_eagerly_outside_functions is deprecated. Please use tf.compat.v1.executing_eagerly_outside_functions instead.\n",
      "\n",
      "5386/5386 [==============================] - 528s 94ms/step - loss: 3.0488 - accuracy: 0.4518\n",
      "Epoch 2/15\n",
      "5386/5386 [==============================] - 527s 98ms/step - loss: 2.1769 - accuracy: 0.5697\n",
      "Epoch 3/15\n",
      "5386/5386 [==============================] - 540s 100ms/step - loss: 1.8674 - accuracy: 0.6134\n",
      "Epoch 4/15\n",
      "5386/5386 [==============================] - 541s 100ms/step - loss: 1.6873 - accuracy: 0.6415\n",
      "Epoch 5/15\n",
      "5386/5386 [==============================] - 412s 77ms/step - loss: 1.5728 - accuracy: 0.6602\n",
      "Epoch 6/15\n",
      "5386/5386 [==============================] - 282s 52ms/step - loss: 1.4932 - accuracy: 0.6742\n",
      "Epoch 7/15\n",
      "5386/5386 [==============================] - 279s 52ms/step - loss: 1.4343 - accuracy: 0.6848\n",
      "Epoch 8/15\n",
      "5386/5386 [==============================] - 294s 55ms/step - loss: 1.3882 - accuracy: 0.6931\n",
      "Epoch 9/15\n",
      "5386/5386 [==============================] - 285s 53ms/step - loss: 1.3493 - accuracy: 0.7002\n",
      "Epoch 10/15\n",
      "2117/5386 [==========>...................] - ETA: 2:55 - loss: 1.3080 - accuracy: 0.7078"
     ]
    }
   ],
   "source": [
    "model.fit([encoder_input_data , decoder_input_data] , decoder_target_data , epochs = 15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ddb5548",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save(r'C:\\Users\\ayush\\jupyter Notebook\\NMT\\code_files\\model.keras')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c9396e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "Common_length_path = r'C:\\Users\\ayush\\jupyter Notebook\\NMT\\code_files\\MAXIMUM_COMMON_LENGTH.txt'\n",
    "with open(Common_length_path, 'w') as file :\n",
    "    file.write(str(COMMON_MAX_LENGTH))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "582562b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder_model = Model(encoder_input, encoder_states)\n",
    "decoder_state_input_h = Input(shape=(64,))\n",
    "decoder_state_input_c= Input(shape=(64,))\n",
    "decoder_states_input= [decoder_state_input_h, decoder_state_input_c]\n",
    "\n",
    "dec_embd2 = decoder_embd(decoder_input)\n",
    "\n",
    "decoder_output2,state_h2, state_c2 = decoder_lstm(dec_embd2, initial_state=decoder_states_input)\n",
    "deccoder_states2= [state_h2, state_c2]\n",
    "\n",
    "decoder_output2 = decoder_dense(decoder_output2)\n",
    "\n",
    "decoder_model = Model(\n",
    "                      [decoder_input]+decoder_states_input,\n",
    "                      [decoder_output2]+ deccoder_states2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c413db6c",
   "metadata": {},
   "source": [
    "## Saving Encoder and Decoder model \n",
    "\n",
    "- This Encode and Decoder model is going to be used in next file which will basically predict sentences\n",
    "- Similarly , I am going to save dictionary based json file for source_word_index and target_word_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "9661a8fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "## saving the encoder model.\n",
    "encoder_model.save(r'C:\\Users\\ayush\\jupyter Notebook\\NMT\\code_files\\encoder_model.keras')\n",
    "## saving the decoder model.\n",
    "decoder_model.save(r'C:\\Users\\ayush\\jupyter Notebook\\NMT\\code_files\\decoder_model.keras')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "44c259f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "word_index_source_path = r'C:\\Users\\ayush\\jupyter Notebook\\NMT\\code_files\\source_word_indices.json'\n",
    "with open(word_index_source_path, 'w') as file :\n",
    "    json.dump(word_indices_source, file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "40637483",
   "metadata": {},
   "outputs": [],
   "source": [
    "word_index_source_path = r'C:\\Users\\ayush\\jupyter Notebook\\NMT\\code_files\\target_word_indices.json'\n",
    "with open(word_index_source_path, 'w') as file :\n",
    "    json.dump(word_indices_target, file)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
